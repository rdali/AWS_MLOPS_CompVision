{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db372b13-8f84-491e-b58e-447a7d32713c",
   "metadata": {},
   "source": [
    "SageMaker End to End Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1753241-ce4e-40a2-be28-a236fddb4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## references:\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d733b4-d3bd-483f-a753-1deaf7b1bd84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker\n",
    "!pip install tensorflow\n",
    "!pip uninstall opencv-python --yes\n",
    "!pip install opencv-python-headless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da5358-e0bc-4b78-b2d6-b6e2ff44997d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## import libraries:\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755dfa4f-1e20-4bfe-8598-c88ee8164e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## set up session:\n",
    "region='us-east-1'\n",
    "\n",
    "sm_sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sm_sess.default_bucket() \n",
    "s3_client = boto3.client(\"s3\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e2937-c910-48a0-8b84-aa615916f9a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define variables:\n",
    "\n",
    "raw_data_uri = 's3://brain-cancer-dataset/raw/'\n",
    "\n",
    "batch_data_uri = 's3://brain-cancer-dataset/data/test/'\n",
    "\n",
    "project_name = 'braincancer-classification'\n",
    "model_name = project_name + '_tf_cnn'\n",
    "\n",
    "training_type = 'ml.t3.medium'\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True,\n",
    "                           expire_after=\"15d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ef9e8-2e90-4af2-b80b-0fd5c1a6228d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## temporary:\n",
    "prc_data_uri = 's3://brain-cancer-dataset/processed/'\n",
    "input_prc_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=prc_data_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786560f7-0301-45cd-a989-586600bc7b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define Pipeline Parameters:\n",
    "region = ParameterString(\n",
    "    name='Region',\n",
    "    default_value='us-east-1'\n",
    ")\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=raw_data_uri,\n",
    ")\n",
    "\n",
    "batch_data = ParameterString(\n",
    "    name=\"BatchData\",\n",
    "    default_value=batch_data_uri,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name='ProcessingInstanceCount',\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name='ProcessingInstanceType',\n",
    "    default_value='ml.t3.medium'\n",
    ")\n",
    "\n",
    "training_instance_count = ParameterInteger(\n",
    "    name='TrainingInstanceCount',\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "training_instance_type = ParameterString(\n",
    "    name='TrainingInstanceType',\n",
    "    default_value='ml.t3.medium'\n",
    ")\n",
    "\n",
    "training_epochs = ParameterString(\n",
    "    name=\"TrainingEpochs\",\n",
    "    default_value=\"10\")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name='ModelApprovalStatus',\n",
    "    default_value='PendingManualApproval'\n",
    "    #default_value='Approved'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66682229-a3bd-41c0-8c22-006879f71136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "#from sagemaker.workflow.pipeline_context import LocalPipelineSession\n",
    "\n",
    "pipeline_session = PipelineSession()\n",
    "#pipeline_session = LocalPipelineSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7827e642-a1fe-4080-9063-68268f8ea6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "\n",
    "#This script should be run on all images prior to training and prediction to standardize images going through the model.\n",
    "# preprocess.py takes in raw MRI images of the Brain (extensions jpg, jpeg and png) and processes them for the machine learning model.  \n",
    "# This processing includes:  \n",
    "# 1- converting images to gray scale and reducing multi-channel images to a single channel  \n",
    "# 2- scaling pixel intensity to a 0-1 scale\n",
    "# 3- cropping black space around the image  \n",
    "# 4- scaling image size to 200 x 200 pixels  \n",
    "# 5- standardizing file extensions  \n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "## install headless opencv due to issue with sagemaker\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python-headless\"])\n",
    "import cv2\n",
    "\n",
    "## functions:\n",
    "\n",
    "def get_imgs(img_path_array):\n",
    "    \"\"\"function that loads images into an array given image paths\"\"\"\n",
    "    imgs = []\n",
    "    for path in img_path_array:\n",
    "        img = cv2.imread(path)\n",
    "        imgs.append(img)\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def zero_runs(arr):\n",
    "    \"\"\"function that detects consecutive zeros\"\"\"\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(arr, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    return ranges\n",
    "\n",
    "\n",
    "def crop_img(img, threshold=0.1):\n",
    "    \"\"\"function that crops an array based on a threshold\"\"\"\n",
    "    img_len, img_width = img.shape\n",
    "    x_min, y_min, x_max, y_max = 0, 0, img_width, img_len\n",
    "    mask = img > threshold\n",
    "    bounds_x = zero_runs(mask.sum(axis=0))\n",
    "    bounds_y = zero_runs(mask.sum(axis=1))\n",
    "    # if there are zero runs, check if they start at 0 and end at image len\n",
    "    if bounds_x.size != 0:\n",
    "        if bounds_x[0][0] == 0:\n",
    "            x_min = bounds_x[0][1]\n",
    "\n",
    "        if bounds_x[-1][1] == x_max:\n",
    "            x_max = bounds_x[-1][0]\n",
    "\n",
    "    if bounds_y.size != 0:\n",
    "        if bounds_y[0][0] == 0:\n",
    "            y_min = bounds_y[0][1]\n",
    "\n",
    "        if bounds_y[-1][1] == y_max:\n",
    "            y_max = bounds_y[-1][0]\n",
    "\n",
    "    return img[y_min:y_max, x_min:x_max]\n",
    "\n",
    "\n",
    "def process_img(img, img_size = (200, 200)):\n",
    "    \"\"\"function that processes the images.\n",
    "    It converts them to gray scale with 1 channel only.\n",
    "    It scales them to 0-1, crops negative space around them and resizes them\"\"\"\n",
    "    img_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_grey_norm = (img_grey / 255)\n",
    "    img_grey_cropped = crop_img(img_grey_norm)\n",
    "    img_grey_std = cv2.resize(img_grey_cropped, img_size)\n",
    "    return img_grey_std\n",
    "\n",
    "## process images and save them in new path\n",
    "\n",
    "def imgs_raw_to_process(imgs_path, output_dir, ext=\".jpg\"):\n",
    "    for img_path in imgs_path:\n",
    "        # load img:\n",
    "        img = cv2.imread(img_path)\n",
    "        # process image\n",
    "        new_img = process_img(img)\n",
    "        # get new file name and path:\n",
    "        old_filename = os.path.basename(img_path)\n",
    "        old_filename_no_ext = \"\".join(list(os.path.splitext(old_filename))[:-1])\n",
    "        new_filename = \"prc_\" + old_filename_no_ext + ext\n",
    "        new_file_path = os.path.join(output_dir, new_filename)\n",
    "        cv2.imwrite(new_file_path, new_img)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"... ... ... Starting Processing Script... ... ...\")\n",
    "    \n",
    "    \n",
    "    base_dir = \"/opt/ml/processing/\"\n",
    "\n",
    "    neg_prc_imgs_path = os.path.join(base_dir, \"output/processed/no\")\n",
    "    pos_prc_imgs_path = os.path.join(base_dir, \"output/processed/yes\")\n",
    "\n",
    "    ## create new folders:\n",
    "    if not os.path.exists(neg_prc_imgs_path):\n",
    "        os.makedirs(neg_prc_imgs_path)\n",
    "\n",
    "    if not os.path.exists(pos_prc_imgs_path):\n",
    "        os.makedirs(pos_prc_imgs_path)\n",
    "        \n",
    "    ## load raw data:\n",
    "    img_extentions = (\".png\", \".jpg\", \".jpeg\")\n",
    "\n",
    "    ## normal scans\n",
    "   \n",
    "    neg_path = os.path.join(base_dir, \"input/no/\")\n",
    "    neg_imgs_path = [os.path.join(neg_path, file) for file in os.listdir(neg_path) if file.lower().endswith(img_extentions)]\n",
    "\n",
    "    ## brain cancer scans\n",
    "    pos_path = os.path.join(base_dir, \"input/yes/\")\n",
    "    pos_imgs_path = [os.path.join(pos_path, file) for file in os.listdir(pos_path) if file.lower().endswith(img_extentions)]\n",
    "    \n",
    "    ## process images and save them in new path\n",
    "    print(\"... ... ... Processing Images... ... ...\")\n",
    "    \n",
    "    imgs_raw_to_process(neg_imgs_path, neg_prc_imgs_path)\n",
    "    imgs_raw_to_process(pos_imgs_path, pos_prc_imgs_path)\n",
    "    \n",
    "    print(\"... ... ... Processing Script Complete... ... ...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e28a07-359e-4d35-bf8e-d2c6de7e7c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"preprocess.py\", Bucket=bucket, Key=f\"{project_name}/code/preprocess.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac236b-a4ba-4300-bda9-5951bff09b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html\n",
    "\n",
    "## define Pre-Processing step\n",
    "\n",
    "sklearn_framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=sklearn_framework_version,\n",
    "    instance_type=training_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=project_name,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "inputs = [\n",
    "    ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(output_name='processed-data', source='/opt/ml/processing/output/processed')\n",
    "]\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name='process-raw-images',\n",
    "    processor=sklearn_processor,\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    code='preprocess.py',\n",
    "    cache_config=cache_config\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c993a5-420f-4d03-b90b-9cdc9b1bc0df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## split data to train, test and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde9d88-4986-4a9c-a3a1-aa5e1b5fd939",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile split.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import boto3\n",
    "from sklearn.model_selection import train_test_split\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "## install headless opencv due to issue with sagemaker\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python-headless\"])\n",
    "import cv2\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"mxnet\"])\n",
    "import mxnet as mx\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tqdm\"])\n",
    "from tqdm import tqdm\n",
    "\n",
    "# method from https://github.com/aws/amazon-sagemaker-examples/blob/main/use-cases/computer_vision/metastases-detection-pipeline.ipynb\n",
    "def write_to_recordio(X: np.ndarray, y: np.ndarray, prefix: str):\n",
    "    record = mx.recordio.MXIndexedRecordIO(idx_path=f\"{prefix}.idx\", uri=f\"{prefix}.rec\", flag=\"w\")\n",
    "    for idx, arr in enumerate(tqdm(X)):\n",
    "        header = mx.recordio.IRHeader(0, y[idx], idx, 0)\n",
    "        s = mx.recordio.pack_img(\n",
    "            header,\n",
    "            arr,\n",
    "            quality=95,\n",
    "            img_fmt=\".jpg\",\n",
    "        )\n",
    "        record.write_idx(idx, s)\n",
    "    record.close()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"... ... ... Starting Processing Script... ... ...\")\n",
    "\n",
    "    region = 'us-east-1'\n",
    "    data_bucket = 'brain-cancer-dataset'\n",
    "    prefix = 'data'\n",
    "    \n",
    "    s3_client = boto3.client(\"s3\", region_name=region)\n",
    "    \n",
    "    img_extention = (\".jpg\")\n",
    "\n",
    "    base_dir = \"/opt/ml/processing/\"\n",
    "\n",
    "    neg_prc_imgs_dir = os.path.join(base_dir, \"input/processed/no\")\n",
    "    pos_prc_imgs_dir = os.path.join(base_dir, \"input/processed/yes\")\n",
    "\n",
    "\n",
    "    ## collect image paths:\n",
    "    neg_imgs_path = [os.path.join(neg_prc_imgs_dir, file) for file in os.listdir(neg_prc_imgs_dir) if file.lower().endswith(img_extention)]\n",
    "    pos_imgs_path = [os.path.join(pos_prc_imgs_dir, file) for file in os.listdir(pos_prc_imgs_dir) if file.lower().endswith(img_extention)]\n",
    "    \n",
    "    ## load negative data:\n",
    "    neg_data = []\n",
    "    for img_path in neg_imgs_path:\n",
    "        img = cv2.imread(img_path)\n",
    "        neg_data.append(img)\n",
    "\n",
    "    neg_labels = [0] * len(neg_data)\n",
    "\n",
    "    ## load positive data:\n",
    "    pos_data = []\n",
    "    for img_path in pos_imgs_path:\n",
    "        img = cv2.imread(img_path)\n",
    "        pos_data.append(img)\n",
    "\n",
    "    pos_labels = [1] * len(pos_data)\n",
    "\n",
    "    ## merge data:\n",
    "\n",
    "    X_data = np.array(neg_data + pos_data)\n",
    "    y_labels = np.array(neg_labels + pos_labels)\n",
    "    \n",
    "    print(\"... ... ... Splitting Images... ... ...\")\n",
    "\n",
    "    ## split data as train and test data:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_labels, test_size=0.3, shuffle=True, random_state=1)\n",
    "\n",
    "    ## further split the test data into test and validate:\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.3, random_state=1)\n",
    "\n",
    "    print(X_train.shape)\n",
    "\n",
    "    output_dir = os.path.join(base_dir, \"output\")\n",
    "    \n",
    "    write_to_recordio(X_train, y_train, prefix=f\"{output_dir}/train/train\")\n",
    "    write_to_recordio(X_val, y_val, prefix=f\"{output_dir}/validation/validation\")\n",
    "    write_to_recordio(X_test, y_test, prefix=f\"{output_dir}/test/test\")\n",
    "    \n",
    "    np.savez(f\"{output_dir}/train/train\", image=X_train, label=y_train)\n",
    "    np.savez(f\"{output_dir}/validation/validation\", image=X_val, label=y_val)\n",
    "    np.savez(f\"{output_dir}/test/test\", image=X_test, label=y_test)\n",
    "    \n",
    "    print(os.listdir(output_dir))\n",
    "    print(os.listdir(f\"{output_dir}/train\"))\n",
    "        \n",
    "    s3_client.upload_file(f\"{output_dir}/train/train.rec\", data_bucket, f\"{prefix}/train/train.rec\")\n",
    "    s3_client.upload_file(f\"{output_dir}/validation/validation.rec\", data_bucket, f\"{prefix}/validation/validation.rec\")\n",
    "    s3_client.upload_file(f\"{output_dir}/test/test.rec\", data_bucket, f\"{prefix}/test/test.rec\")\n",
    "    s3_client.upload_file(f\"{output_dir}/train/train.npz\", data_bucket, f\"{prefix}/train/train.npz\")\n",
    "    s3_client.upload_file(f\"{output_dir}/validation/validation.npz\", data_bucket, f\"{prefix}/validation/validation.npz\")\n",
    "    s3_client.upload_file(f\"{output_dir}/test/test.npz\", data_bucket, f\"{prefix}/test/test.npz\")\n",
    "    \n",
    "    print(\"... ... ... Processing Script Complete... ... ...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e064614-64fd-4e61-93d6-03072e1c9686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"split.py\", Bucket=bucket, Key=f\"{project_name}/code/split.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7bef0-3464-4e2c-a1ac-a0b10b5b95c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ProcessingInput(source=step_process.properties.ProcessingOutputConfig.Outputs['processed-data'].S3Output.S3Uri,\n",
    "                    #source=input_prc_data,\n",
    "                    destination=\"/opt/ml/processing/input/processed\"),\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "        ProcessingOutput(\n",
    "                output_name='train',\n",
    "                source='/opt/ml/processing/output/train'),\n",
    "        ProcessingOutput(\n",
    "                output_name='validation',\n",
    "                source='/opt/ml/processing/output/validation'),\n",
    "        ProcessingOutput(\n",
    "                output_name='test',\n",
    "                source='/opt/ml/processing/output/test')\n",
    "\n",
    "]\n",
    "\n",
    "step_split_dataset = ProcessingStep(\n",
    "    name='split-prc-images',\n",
    "    processor=sklearn_processor,\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    code='split.py',\n",
    "    cache_config=cache_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b6ee9-0235-4f36-8241-f1d19dbd2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure Model Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2383611-ef22-45d9-aab5-270b072e6f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html\n",
    "# supported insances: P2, P3, G4dn, and G5 instances\n",
    "\n",
    "num_training_samples = 174 ## should not be hard coded\n",
    "\n",
    "estimator_image = sagemaker.image_uris.retrieve(\"image-classification\", region)\n",
    "\n",
    "# hyperparameter docs: https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html\n",
    "hyperparameters = {\n",
    "    \"use_pretrained_model\": 1,\n",
    "    \"augmentation_type\": \"crop_color_transform\",\n",
    "    \"image_shape\": \"3,200,200\",\n",
    "    \"num_classes\": 2,\n",
    "    \"mini_batch_size\": 10,\n",
    "    \"epochs\": 500,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"precision_dtype\": \"float32\",\n",
    "    \"num_training_samples\": num_training_samples,\n",
    "    #\"early_stopping\": True,\n",
    "    #\"early_stopping_min_epochs\": 150,\n",
    "    #\"early_stopping_patience\": 30,\n",
    "}\n",
    "\n",
    "estimator_config = {\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"image_uri\": estimator_image,\n",
    "    \"role\": role,\n",
    "    \"instance_count\": 1,\n",
    "    \"instance_type\": \"ml.p3.2xlarge\",\n",
    "    \"output_path\": f\"s3://{bucket}/{project_name}/training_jobs\",\n",
    "    #\"use_spot_instances\": True,\n",
    "    #\"max_wait\": 4000,\n",
    "    #\"max_run\": 3600,\n",
    "    \"disable_profiler\": True,\n",
    "}\n",
    "\n",
    "image_classifier = sagemaker.estimator.Estimator(**estimator_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad80c5b2-e34c-4e8d-81b0-c0602d6df8f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## train Model Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b1297-8e8b-4e6a-98d2-a4885270af0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_bucket = 'brain-cancer-dataset'\n",
    "\n",
    "step_train_inputs = {\n",
    "    \"train\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=step_split_dataset.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "        #s3_data=f\"s3://{data_bucket}/data/training/training.rec\",\n",
    "        content_type=\"application/x-recordio\",\n",
    "    ),\n",
    "    \"validation\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=step_split_dataset.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "        #s3_data=f\"s3://{data_bucket}/data/validation/validation.rec\",\n",
    "        content_type=\"application/x-recordio\",\n",
    "    )\n",
    "}\n",
    "\n",
    "step_train = TrainingStep(name=\"train-model\",\n",
    "                          estimator=image_classifier,\n",
    "                          inputs=step_train_inputs,\n",
    "                          cache_config=cache_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6e20a-b4fd-41ae-b49a-c99f781ee368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## error with using direct dependencies:\n",
    "#ClientError: Failed to invoke sagemaker:CreateTrainingJob. Error Details: No S3 objects found under S3 URL \"s3://sagemaker-us-east-1-969844977961/braincancer-classification-pipeline/2pug0h6dp8tk/split-prc-images/output/training\" given in input data source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e821675-ba86-4a16-a541-2344e28dc706",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b7a14-de06-4547-88ff-bb5b724e52d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate.py\n",
    "\n",
    "## inspired by https://github.com/aws-samples/amazon-sagemaker-pipelines-mxnet-image-classification/blob/main/scripts/evaluate.py\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import pickle\n",
    "import tarfile\n",
    "import glob\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"mxnet\"])\n",
    "import mxnet as mx\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    logger.debug(\"... ... ...Starting evaluation Script... ... ...\")\n",
    "    \n",
    "    ###### 1. Loading trained MXNet model ######\n",
    "    logger.debug(\"... ... ...Loading MXNet model... ... ...\")\n",
    "    \n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "        \n",
    "    ## load model:\n",
    "    param_file = glob.glob('./*.params')\n",
    "    epoch = int(param_file[0][-11:-7])\n",
    "    sym, arg_params, aux_params = mx.model.load_checkpoint(\"image-classification\", epoch)\n",
    "\n",
    "    model_shapes_json_file = open('./model-shapes.json')\n",
    "    model_shapes_dict = json.load(model_shapes_json_file)[0]\n",
    "    train_batch_size = model_shapes_dict['shape'][0]\n",
    "    train_data_shape = tuple(model_shapes_dict['shape'][1:])\n",
    "\n",
    "\n",
    "    logger.debug(\"... ... ...Loading test data... ... ...\")\n",
    "    \n",
    "    test_path = \"/opt/ml/processing/test/test.rec\"\n",
    "\n",
    "    test = mx.io.ImageRecordIter(path_imgrec=test_path,\n",
    "                                 data_name='data',\n",
    "                                 label_name='softmax_label',\n",
    "                                 batch_size=train_batch_size,\n",
    "                                 data_shape=train_data_shape,\n",
    "                                 rand_crop=False,\n",
    "                                 rand_mirro=False)\n",
    "\n",
    "    ###### 3. Making predictions on the test set ######\n",
    "\n",
    "\n",
    "    mod = mx.mod.Module(symbol=sym, context=mx.cpu())\n",
    "\n",
    "    mod.bind(for_training=False,\n",
    "             data_shapes=test.provide_data,\n",
    "             label_shapes=test.provide_label)\n",
    "\n",
    "    mod.set_params(arg_params, aux_params)\n",
    "\n",
    "    ###### 4. Calculating evaluation metrics ######\n",
    "\n",
    "\n",
    "    metric = mod.score(eval_data=test, eval_metric=['acc', 'f1'])\n",
    "    test_accuracy = metric[0][1]\n",
    "    test_f1 = metric[1][1]\n",
    "\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    print(f\"Test F1: {test_f1}\")\n",
    "\n",
    "    report_dict = {\n",
    "        \"classification_metrics\": {\n",
    "            \"accuracy\": {\n",
    "                \"value\": test_accuracy\n",
    "            },\n",
    "            \"f1\": {\n",
    "                \"value\": test_f1\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    ###### 5. Saving evaluation metrics to output path ######\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))\n",
    "\n",
    "    print(\"... ... ... Processing Script Complete... ... ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e51e3-15fc-4ecd-82c4-e9a01b04ef5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"evaluate.py\", Bucket=bucket, Key=f\"{project_name}/code/evaluate.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3da2e-c032-4ba6-9f71-ccb8b754d51f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluate_model_processor = SKLearnProcessor(\n",
    "    framework_version=sklearn_framework_version,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{project_name}-evaluate\",\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "#https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-propertyfile.html\n",
    "eval_report = PropertyFile(\n",
    "    name=\"eval-report\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "# Use the evaluate_model_processor in a Sagemaker pipelines ProcessingStep.\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"eval-model\",\n",
    "    processor=evaluate_model_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            #source=\"s3://brain-cancer-dataset/braincancer-classification/training_jobs/pipelines-b4fizf9t1tha-train-model-m6jbzJhU1H/output/model.tar.gz\",\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_split_dataset.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            #source=f\"s3://{data_bucket}/data/testing\",\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"evaluate.py\",\n",
    "    property_files=[eval_report],\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf8ca5-6e83-473a-8402-78d26a00c97c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create Model: By creating a model, you tell SageMaker where it can find the model components.\n",
    "\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "# Create a SageMaker model\n",
    "\n",
    "model = Model(\n",
    "    image_uri=estimator_image,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "# Define the model input for your SageMaker model\n",
    "\n",
    "model_inputs = CreateModelInput(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    accelerator_type=\"ml.eia1.medium\",\n",
    ")\n",
    "\n",
    "# Create CreateModelStep:\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"create-model\",\n",
    "    model=model,\n",
    "    inputs=model_inputs\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fc296-c165-4d7d-b777-486173be387b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Register a Model:\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "step_register_model = RegisterModel(\n",
    "    name=\"register-model\",\n",
    "    estimator=image_classifier,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"image/jpeg\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=project_name,\n",
    "    model_metrics=model_metrics,\n",
    "    approval_status=model_approval_status,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34597af-6670-40f7-bcac-2d1b8eb32544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deploy lambda to slack function:\n",
    "# https://serverlessrepo.aws.amazon.com/applications/arn:aws:serverlessrepo:us-east-1:289559741701:applications~lambda-to-slack\n",
    "# edit IAM policy to allow the role used to invoke the given lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a02e8-2480-4f3c-bc8e-3365107cafb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile invoke_lambda.py\n",
    "\n",
    "import boto3\n",
    "import argparse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"... ... ... Starting Processing Script... ... ...\")\n",
    "\n",
    "    lambda_client = boto3.client('lambda', region_name='us-east-1')\n",
    "    \n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--status\", type=str)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    status = args.status\n",
    "\n",
    "    lambda_payload_negative = b\"\"\"[\"Brain Classification Pipeline has stopped. Trained Model did not pass accuracy threshold!\"]\"\"\"\n",
    "    lambda_payload_positive = b\"\"\"[\"Brain Classification Pipeline has Completed Successfully!\"]\"\"\"\n",
    "    \n",
    "    if status == \"pass\":\n",
    "        lambda_payload = lambda_payload_positive\n",
    "    else:\n",
    "        lambda_payload = lambda_payload_negative\n",
    "    \n",
    "    \n",
    "    lambda_client.invoke(FunctionName='arn:aws:lambda:us-east-1:969844977961:function:serverlessrepo-lambda-to-slack-LambdaToSlack-JGcKnm7LElCz', \n",
    "                         InvocationType='Event',\n",
    "                         Payload=lambda_payload)\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039ede4-0d0e-4c0d-9d78-0b4e9d2d5489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"invoke_lambda.py\", Bucket=bucket, Key=f\"{project_name}/code/invoke_lambda.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2279a-79ec-4dd5-8fe8-e6c6a16f9956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "invoke_lambda_script_uri = f\"s3://{bucket}/{project_name}/code/invoke_lambda.py\"\n",
    "\n",
    "lambda_processor = SKLearnProcessor(\n",
    "    framework_version=sklearn_framework_version,\n",
    "    role=role,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{project_name}-invoke-lambda\",\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "step_lambda_pass = ProcessingStep(\n",
    "    name=\"invoke-lambda-pass\",\n",
    "    processor=lambda_processor,\n",
    "    job_arguments=[\n",
    "        \"--status\",\n",
    "        \"pass\"\n",
    "    ],\n",
    "    code=invoke_lambda_script_uri,\n",
    "    cache_config=cache_config\n",
    ")\n",
    "\n",
    "step_lambda_fail = ProcessingStep(\n",
    "    name=\"invoke-lambda-fail\",\n",
    "    processor=lambda_processor,\n",
    "    job_arguments=[\n",
    "        \"--status\",\n",
    "        \"fail\"\n",
    "    ],\n",
    "    code=invoke_lambda_script_uri,\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df8724-0169-4705-a987-ba6c84296efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create condition to register the model if accuracy > 0.6\n",
    "\n",
    "acceptance_thresold=0.6\n",
    "\n",
    "condition_acc = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=eval_report,\n",
    "        json_path=\"classification_metrics.accuracy.value\"\n",
    "    ),\n",
    "    right=acceptance_thresold,\n",
    ")\n",
    "\n",
    "# This step encompasses 'step_register' and only performs the 'step_register' if the model accuracy is greater than 0.70\n",
    "step_condition = ConditionStep(\n",
    "    name=\"model-accuracy-check\",\n",
    "    conditions=[condition_acc],\n",
    "    if_steps=[step_register_model, step_create_model],\n",
    "    else_steps=[step_lambda_fail],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655cdef-18f9-4c8f-931d-a24b99b2b2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile deploy_model.py\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "\n",
    "# Parse argument variables passed via the DeployModel processing step\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model-name\", type=str)\n",
    "parser.add_argument(\"--region\", type=str)\n",
    "parser.add_argument(\"--endpoint-instance-type\", type=str)\n",
    "parser.add_argument(\"--endpoint-name\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "region = args.region\n",
    "boto3.setup_default_session(region_name=region)\n",
    "sagemaker_boto_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# name truncated per sagameker length requirements (63 char max)\n",
    "endpoint_config_name = f\"{args.model_name[:56]}-config\"\n",
    "existing_configs = sagemaker_boto_client.list_endpoint_configs(NameContains=endpoint_config_name)[\n",
    "    \"EndpointConfigs\"\n",
    "]\n",
    "\n",
    "if not existing_configs:\n",
    "    create_ep_config_response = sagemaker_boto_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                \"InstanceType\": args.endpoint_instance_type,\n",
    "                \"InitialVariantWeight\": 1,\n",
    "                \"InitialInstanceCount\": 1,\n",
    "                \"ModelName\": args.model_name,\n",
    "                \"VariantName\": \"AllTraffic\",\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "existing_endpoints = sagemaker_boto_client.list_endpoints(NameContains=args.endpoint_name)[\n",
    "    \"Endpoints\"\n",
    "]\n",
    "\n",
    "if not existing_endpoints:\n",
    "    create_endpoint_response = sagemaker_boto_client.create_endpoint(\n",
    "        EndpointName=args.endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "    )\n",
    "\n",
    "endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=args.endpoint_name)\n",
    "endpoint_status = endpoint_info[\"EndpointStatus\"]\n",
    "\n",
    "while endpoint_status == \"Creating\":\n",
    "    endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=args.endpoint_name)\n",
    "    endpoint_status = endpoint_info[\"EndpointStatus\"]\n",
    "    print(\"Endpoint status:\", endpoint_status)\n",
    "    if endpoint_status == \"Creating\":\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c377699-a9cc-4ad8-8f2a-ce22450f73e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"deploy_model.py\", Bucket=bucket, Key=f\"{project_name}/code/deploy_model.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e1528d-aa70-4fd9-9a95-63931e474a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## deploy model: edited from: https://github.com/aws/amazon-sagemaker-examples/blob/main/use-cases/computer_vision/deploy_model.py\n",
    "\n",
    "\n",
    "deploy_model_script_uri = f\"s3://{bucket}/{project_name}/code/deploy_model.py\"\n",
    "deploy_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "deploy_model_processor = SKLearnProcessor(\n",
    "    framework_version=sklearn_framework_version,\n",
    "    role=role,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{project_name}-deploy-model\",\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "step_deploy = ProcessingStep(\n",
    "    name=\"model-deploy\",\n",
    "    processor=deploy_model_processor,\n",
    "    job_arguments=[\n",
    "        \"--model-name\",\n",
    "        step_create_model.properties.ModelName,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-instance-type\",\n",
    "        deploy_instance_type,\n",
    "        \"--endpoint-name\",\n",
    "        f\"{project_name}-model-pipeline\",\n",
    "    ],\n",
    "    code=deploy_model_script_uri,\n",
    "    cache_config=cache_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d83bafb3-97a9-49d2-8280-1fd782d0559d",
   "metadata": {},
   "source": [
    "## Define a TransformStep to Perform Batch Transformation: \n",
    "\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "\n",
    "## Create a transformer instance:\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=f\"s3://{bucket}/{project_name}/transfomer\",\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "## Create a TransformStep:\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"transform\",\n",
    "    transformer=transformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e4e4b-a1af-4adc-8945-7b3bf3b426c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%writefile model_monitor.py\n",
    "\n",
    "#https://github.com/aws/amazon-sagemaker-examples/blob/4d1558a847ffbccde6b68e05ca2ee8198ce2fee9/end_to_end/music_recommendation/code/model_monitor.py\n",
    "## Monitor Deployed Model: https://github.com/aws/amazon-sagemaker-examples/blob/4d1558a847ffbccde6b68e05ca2ee8198ce2fee9/end_to_end/music_recommendation/end_to_end_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9635529-7656-4a89-89f0-7759ea843da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## to set up dependencies manually:\n",
    "#step_train.add_depends_on([step_split_dataset])\n",
    "step_lambda_pass.add_depends_on([step_deploy, step_register_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0414c3-d7e9-4343-92fc-0556879881ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create the pipeline: https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = project_name + \"-pipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        region,\n",
    "        processing_instance_type, \n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        training_instance_count,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        model_name,\n",
    "        training_epochs,\n",
    "    ],\n",
    "    steps=[step_process, step_split_dataset, step_train, step_eval, step_condition, step_lambda_pass, step_deploy]\n",
    "    #steps=[step_process, step_split_dataset, step_train, step_eval, step_create_model, step_condition, step_deploy]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a8749-33f9-460e-ad19-3c9645f4649f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Submit pipeline\n",
    "\n",
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37e1d1-bc0d-4d96-b010-7dd35612d107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Execute Pipeline:\n",
    "\n",
    "execution = pipeline.start()\n",
    "\n",
    "\n",
    "## to change default params:\n",
    "\n",
    "#execution = pipeline.start(\n",
    "#    parameters=dict(\n",
    "#        TrainingEpochs=1,\n",
    "#        ModelName='another-name',\n",
    "#        ModelApprovalStatus='Approved'\n",
    "#    )\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cad34f-9c62-4eba-a7d3-13e41c7f5169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## check pipeline steps:\n",
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030828ac-71dc-4027-8929-ba7e4f834d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## describe pipeline execution\n",
    "\n",
    "execution.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83cb4ee-f0fd-4ce0-9103-d45dbf09aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## wait for completion:\n",
    "#execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed436fc-9b55-47d6-9a37-b1c23ad7274b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Lineage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1118a-a78c-464d-b373-156ea0d9b43d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Reference: https://github.com/aws/amazon-sagemaker-examples/blob/main/use-cases/computer_vision/metastases-detection-pipeline.ipynb\n",
    "\n",
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "viz = LineageTableVisualizer(sm_sess)\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    pprint(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531e523-9d8f-41a9-971e-dd2a39a335cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## inference example: https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-fulltraining.html#Batch-transform\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "runtime = boto3.Session().client(service_name=\"runtime.sagemaker\")\n",
    "\n",
    "endpoint_name = \"braincancer-classification-model-pipeline\"\n",
    "object_categories = [\n",
    "    \"Normal\",\n",
    "    \"Cancer\"]\n",
    "\n",
    "def infer_braincancer(file_name, endpoint_name, object_categories):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        payload = f.read()\n",
    "        payload = bytearray(payload)\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/x-image\", Body=payload\n",
    "    )\n",
    "    result = response[\"Body\"].read()\n",
    "    # result will be in json format and convert it to ndarray\n",
    "    result = json.loads(result)\n",
    "    print(result)\n",
    "    # the result will output the probabilities for all classes\n",
    "    # find the class with maximum probability and print the class index\n",
    "    index = np.argmax(result)\n",
    "    print(\"Result: label - \" + object_categories[index] + \", probability - \" + str(result[index]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb531bc2-0d7a-40dd-9e40-f2a23a19da49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test cancer brain image\n",
    "\n",
    "from IPython.display import Image\n",
    "file_name_c = \"prc_Y1.jpg\"\n",
    "\n",
    "## test cancer image:\n",
    "print(\"Testing Cancer image:\")\n",
    "infer_braincancer(file_name_c, endpoint_name, object_categories)\n",
    "Image(file_name_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c30a4ca-177a-404a-b556-3cc9f6f33399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test normal brain image:\n",
    "\n",
    "file_name_n = \"prc_11 no.jpg\"\n",
    "\n",
    "## test Normal image:\n",
    "print(\"Testing Normal image:\")\n",
    "infer_braincancer(file_name_n, endpoint_name, object_categories)\n",
    "Image(file_name_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e560b-7764-4f64-a486-07f6078672d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean Resources: functions from https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb\n",
    "\n",
    "def delete_model_package_group(sm_client, package_group_name):\n",
    "    try:\n",
    "        model_versions = sm_client.list_model_packages(ModelPackageGroupName=package_group_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "        return\n",
    "\n",
    "    for model_version in model_versions[\"ModelPackageSummaryList\"]:\n",
    "        try:\n",
    "            sm_client.delete_model_package(ModelPackageName=model_version[\"ModelPackageArn\"])\n",
    "        except Exception as e:\n",
    "            print(\"{} \\n\".format(e))\n",
    "        time.sleep(0.5)  # Ensure requests aren't throttled\n",
    "\n",
    "    try:\n",
    "        sm_client.delete_model_package_group(ModelPackageGroupName=package_group_name)\n",
    "        print(\"{} model package group deleted\".format(package_group_name))\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "    return\n",
    "\n",
    "\n",
    "def delete_sagemaker_pipeline(sm_client, pipeline_name):\n",
    "    try:\n",
    "        sm_client.delete_pipeline(\n",
    "            PipelineName=pipeline_name,\n",
    "        )\n",
    "        print(\"{} pipeline deleted\".format(pipeline_name))\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6bcc03-0d62-43b9-ba05-9da0a8ca63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete_model_package_group(client, model_package_group_name)\n",
    "#delete_sagemaker_pipeline(client, pipeline_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44c3e4ea-b82b-442f-8627-d274edc8b2ed",
   "metadata": {},
   "source": [
    "## example cell: Esc + r. To turn back on: Esc + y\n",
    "## PySpark Processor: https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing\n",
    "\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "pyspark_processor = PySparkProcessor(\n",
    "    framework_version='2.4',\n",
    "    role=role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "step_args = pyspark_processor.run(\n",
    "    inputs=[ProcessingInput(source=<input_data>, destination=\"/opt/ml/processing/input\"),],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\")\n",
    "    ],\n",
    "    code=\"preprocess.py\",\n",
    "    arguments=None,\n",
    ")\n",
    "\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"spark-process\",\n",
    "    step_args=step_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a010a-8af1-4f9a-a10c-114a37da06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distributed Training: https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html\n",
    "# Doesn't seem supported for MXNet Models? https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-model-parallel-support.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2045c872-aa6e-45f4-9c42-a9affad89a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## multi-model endpoints: https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/multi_model_linear_learner_home_value/linear_learner_multi_model_endpoint_inf_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772e8ea-2705-4d89-a6f0-1c17aaba4032",
   "metadata": {},
   "source": [
    "## imporvements:\n",
    "\n",
    "- IaC\n",
    "- Version Control   \n",
    "- Linting and formating   \n",
    "- Lambda uses a processor instead of the lambda Step   \n",
    "- Some values like num_training_samples are hardcoded. These should be infered from data and stored somewhere\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
